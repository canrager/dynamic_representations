{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440f89df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/can/dynamic_representations/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import asdict, dataclass\n",
    "import os\n",
    "import torch as th\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import gc\n",
    "\n",
    "from src.model_utils import load_nnsight_model, load_sae\n",
    "from src.exp_utils import compute_llm_artifacts\n",
    "from src.cache_utils import batch_sae_cache\n",
    "\n",
    "from src.configs import *\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CacheConfig:\n",
    "    llm: LLMConfig\n",
    "    env: EnvironmentConfig\n",
    "\n",
    "cfg = CacheConfig(\n",
    "    llm=GEMMA2_LLM_CFG,\n",
    "    env=ENV_CFG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a517887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma2ForCausalLM(\n",
      "  (model): Gemma2Model(\n",
      "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-25): 26 x Gemma2DecoderLayer(\n",
      "        (self_attn): Gemma2Attention(\n",
      "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
      "        )\n",
      "        (mlp): Gemma2MLP(\n",
      "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
      "          (act_fn): GELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n",
      "Gemma2Config {\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 2304,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9216,\n",
      "  \"layer_types\": [\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, submodule, hidden_dim, num_heads = load_nnsight_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaad8761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma2ForCausalLM(\n",
      "  (model): Gemma2Model(\n",
      "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-25): 26 x Gemma2DecoderLayer(\n",
      "        (self_attn): Gemma2Attention(\n",
      "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
      "        )\n",
      "        (mlp): Gemma2MLP(\n",
      "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
      "          (act_fn): GELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f74a6cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Gemma2ForCausalLM(\n  (model): Gemma2Model(\n    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n    (layers): ModuleList(\n      (0-25): 26 x Gemma2DecoderLayer(\n        (self_attn): Gemma2Attention(\n          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n        )\n        (mlp): Gemma2MLP(\n          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n          (act_fn): GELUTanh()\n        )\n        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n      )\n    )\n    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n    (rotary_emb): Gemma2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n  (generator): Generator(\n    (streamer): Streamer()\n  )\n) has no attribute layers",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dynamic_representations/.venv/lib/python3.12/site-packages/nnsight/intervention/envoy.py:1050\u001b[39m, in \u001b[36mEnvoy.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1048\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1050\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: Gemma2ForCausalLM(\n  (model): Gemma2Model(\n    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n    (layers): ModuleList(\n      (0-25): 26 x Gemma2DecoderLayer(\n        (self_attn): Gemma2Attention(\n          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n        )\n        (mlp): Gemma2MLP(\n          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n          (act_fn): GELUTanh()\n        )\n        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n      )\n    )\n    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n    (rotary_emb): Gemma2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n  (generator): Generator(\n    (streamer): Streamer()\n  )\n) has no attribute layers"
     ]
    }
   ],
   "source": [
    "print(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4f944c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0-25): 26 x Gemma2DecoderLayer(\n",
      "    (self_attn): Gemma2Attention(\n",
      "      (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
      "      (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "      (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "      (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
      "    )\n",
      "    (mlp): Gemma2MLP(\n",
      "      (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "      (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "      (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
      "      (act_fn): GELUTanh()\n",
      "    )\n",
      "    (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "    (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "    (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "    (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a78c227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              * @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
      "                              0 def forward(\n",
      "                              1     self,\n",
      "                              2     hidden_states: torch.Tensor,\n",
      "                              3     position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
      "                              4     attention_mask: Optional[torch.Tensor],\n",
      "                              5     past_key_values: Optional[Cache] = None,\n",
      "                              6     cache_position: Optional[torch.LongTensor] = None,\n",
      "                              7     **kwargs: Unpack[FlashAttentionKwargs],\n",
      "                              8 ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n",
      "                              9     input_shape = hidden_states.shape[:-1]\n",
      "                             10     hidden_shape = (*input_shape, -1, self.head_dim)\n",
      "                             11 \n",
      " self_q_proj_0            -> 12     query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      " view_0                   ->  +     ...\n",
      " transpose_0              ->  +     ...\n",
      " self_k_proj_0            -> 13     key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      " view_1                   ->  +     ...\n",
      " transpose_1              ->  +     ...\n",
      " self_v_proj_0            -> 14     value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      " view_2                   ->  +     ...\n",
      " transpose_2              ->  +     ...\n",
      "                             15 \n",
      "                             16     cos, sin = position_embeddings\n",
      " apply_rotary_pos_emb_0   -> 17     query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
      "                             18 \n",
      "                             19     if past_key_values is not None:\n",
      "                             20         # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
      "                             21         cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
      " past_key_values_update_0 -> 22         key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
      "                             23 \n",
      "                             24     attention_interface: Callable = eager_attention_forward\n",
      "                             25     if self.config._attn_implementation != \"eager\":\n",
      "                             26         attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
      "                             27 \n",
      " attention_interface_0    -> 28     attn_output, attn_weights = attention_interface(\n",
      "                             29         self,\n",
      "                             30         query_states,\n",
      "                             31         key_states,\n",
      "                             32         value_states,\n",
      "                             33         attention_mask,\n",
      "                             34         dropout=self.attention_dropout if self.training else 0.0,\n",
      "                             35         scaling=self.scaling,\n",
      "                             36         sliding_window=self.sliding_window,\n",
      "                             37         softcap=self.attn_logit_softcapping,\n",
      "                             38         **kwargs,\n",
      "                             39     )\n",
      "                             40 \n",
      " attn_output_reshape_0    -> 41     attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
      " contiguous_0             ->  +     ...\n",
      " self_o_proj_0            -> 42     attn_output = self.o_proj(attn_output)\n",
      "                             43     return attn_output, attn_weights\n",
      "                             44 \n"
     ]
    }
   ],
   "source": [
    "print(model.model.layers[0].self_attn.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ec0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 7, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with th.inference_mode():\n",
    "    with model.trace(\"Hello, I am Can!\", scan=False, validate=False, output_attentions=True):\n",
    "        attn_pattern_BAPP = submodule.self_attn.output[1].save()\n",
    "        out = submodule.output\n",
    "        if isinstance(out, tuple):\n",
    "            out = out[0]\n",
    "        out.save()\n",
    "attn_pattern_APP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d300cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamic_representations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
